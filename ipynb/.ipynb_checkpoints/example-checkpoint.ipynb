{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quant Aware Training Example (tf.__version__=1.13.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# tf.enable_eager_execution()\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((?, 192, 160, 3), (?, 40)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_DATASET = 1000\n",
    "\n",
    "def load_labels(labels_path):\n",
    "    f = open(labels_path)\n",
    "    # line 1: number of images\n",
    "    num_imgs = int(f.readline())\n",
    "    # line 2: attribute names, 40 in total\n",
    "    attr_names = f.readline().split()\n",
    "    # line 3 to end: 00xx.jpg -1 1 -1 1 ...\n",
    "    labels = []\n",
    "    for i in range(NUM_DATASET):\n",
    "        labels.append(list(map(np.float32, f.readline().split()[1:])))\n",
    "    labels = np.array(labels)\n",
    "    labels[labels<0] = 0\n",
    "    return labels\n",
    "\n",
    "def load_imgs(imgs_dir):\n",
    "    img_paths = os.listdir(imgs_dir)\n",
    "    img_paths.sort()\n",
    "    img_paths = img_paths[:NUM_DATASET]\n",
    "    for i in range(len(img_paths)):\n",
    "        img_paths[i] = os.path.join(imgs_dir,img_paths[i])\n",
    "    return img_paths\n",
    "\n",
    "def preprocess(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # uint8 range: [0,255]\n",
    "    img = tf.image.resize(img, [192, 160])\n",
    "    # new range: [-128,127]\n",
    "    img -= 128\n",
    "    return img    \n",
    "\n",
    "# def parse(x):\n",
    "#     result = tf.parse_tensor(x, out_type=tf.int8)\n",
    "#     result = tf.reshape(result, (192,160,3))\n",
    "#     return result\n",
    "\n",
    "# if os.path.exists('../dataset/tfrec') == False:\n",
    "#     os.mkdir('../dataset/tfrec')\n",
    "#     imgs_dir  = '../dataset/img_align_celeba'\n",
    "#     img_paths = load_imgs(imgs_dir)\n",
    "#     ds_imgs = tf.data.Dataset.from_tensor_slices(img_paths).map(tf.io.read_file)\n",
    "#     ds_imgs_serialized = ds_imgs.map(tf.serialize_tensor)\n",
    "#     tfrec = tf.data.experimental.TFRecordWriter('../dataset/tfrec/imgs.tfrec')\n",
    "#     tfrec.write(ds_imgs_serialized)\n",
    "# else: \n",
    "#     ds_imgs = tf.data.TFRecordDataset('../dataset/tfrec/imgs.tfrec')\n",
    "#     ds_imgs = ds_imgs.map(parse, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "imgs_dir = '../dataset/celeba/img_align_celeba'\n",
    "img_paths = load_imgs(imgs_dir)\n",
    "ds_imgs = tf.data.Dataset.from_tensor_slices(img_paths).map(preprocess)\n",
    "\n",
    "labels_path = '../dataset/celeba/list_attr_celeba.txt'\n",
    "ds_labels = tf.data.Dataset.from_tensor_slices(load_labels(labels_path))\n",
    "\n",
    "ds_celeba = tf.data.Dataset.zip((ds_imgs,ds_labels))\n",
    "ds_celeba = ds_celeba.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=8192))\n",
    "ds_celeba = ds_celeba.batch(32).prefetch(AUTOTUNE)\n",
    "ds_celeba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a Mobilenet (V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenet_v1(tensor_in, num_classes, depth_multiplier, is_training):\n",
    "    \"\"\"\n",
    "    Constructs a Mobilenet V1 base convnet\n",
    "    \n",
    "    Args:\n",
    "        tensor_in: a tensor of shape [NHWC]\n",
    "        num_classes: number of channels of the final dense layer\n",
    "        depth_multiplier: multiplier for number of channels, \n",
    "            should be 0.25, 0.5, 0.75 or 1.0\n",
    "        is_training: the model is constructed for training or not\n",
    "        \n",
    "    Returns:\n",
    "        logits: output tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # list of dicts specifying the base net architecture\n",
    "    MOBILENET_V1_BASE_DEFS = [\n",
    "        {'layer':'conv2d', 'name':'Conv_0',  'stride':2, 'depth':32  },\n",
    "        {'layer':'convds', 'name':'Conv_1',  'stride':1, 'depth':64  },\n",
    "        {'layer':'convds', 'name':'Conv_2',  'stride':2, 'depth':128 },\n",
    "        {'layer':'convds', 'name':'Conv_3',  'stride':1, 'depth':128 },\n",
    "        {'layer':'convds', 'name':'Conv_4',  'stride':2, 'depth':256 },\n",
    "        {'layer':'convds', 'name':'Conv_5',  'stride':1, 'depth':256 },\n",
    "        {'layer':'convds', 'name':'Conv_6',  'stride':2, 'depth':512 },\n",
    "        {'layer':'convds', 'name':'Conv_7',  'stride':1, 'depth':512 },\n",
    "        {'layer':'convds', 'name':'Conv_8',  'stride':1, 'depth':512 },\n",
    "        {'layer':'convds', 'name':'Conv_9',  'stride':1, 'depth':512 },\n",
    "        {'layer':'convds', 'name':'Conv_10', 'stride':1, 'depth':512 },\n",
    "        {'layer':'convds', 'name':'Conv_11', 'stride':1, 'depth':512 },\n",
    "        {'layer':'convds', 'name':'Conv_12', 'stride':2, 'depth':1024},\n",
    "        {'layer':'convds', 'name': 'Conv_13', 'stride':1, 'depth':1024}\n",
    "    ]\n",
    "    \n",
    "    # hyperparams to use\n",
    "    activation_fn = tf.nn.relu6\n",
    "    normalizer_fn=tf.contrib.slim.batch_norm\n",
    "    normalizer_params = {\n",
    "        'is_training': is_training,\n",
    "        'center': True, \n",
    "        'scale': True, \n",
    "        'decay': 0.9997, \n",
    "        'epsilon': 0.001, \n",
    "        'updates_collections': tf.GraphKeys.UPDATE_OPS\n",
    "    }\n",
    "    weights_initializer = tf.truncated_normal_initializer(stddev=0.09)\n",
    "    weights_regularizer = tf.contrib.layers.l2_regularizer(0.00004)\n",
    "    \n",
    "    with tf.variable_scope('MobilenetV1', [tensor_in]):\n",
    "        net = tensor_in\n",
    "        # conv layers\n",
    "        for layer_def in MOBILENET_V1_BASE_DEFS:\n",
    "            if layer_def['layer']=='conv2d':\n",
    "                net = tf.contrib.slim.conv2d( net,\n",
    "                                              num_outputs=layer_def['depth']*depth_multiplier,\n",
    "                                              kernel_size=[3,3],\n",
    "                                              stride=layer_def['stride'],\n",
    "                                              activation_fn=activation_fn,\n",
    "                                              normalizer_fn=normalizer_fn,\n",
    "                                              normalizer_params=normalizer_params,\n",
    "                                              weights_initializer=weights_initializer,\n",
    "                                              weights_regularizer=weights_regularizer,\n",
    "                                              scope=layer_def['name'])\n",
    "            elif layer_def['layer'] == 'convds':\n",
    "                # depthwise conv\n",
    "                net = tf.contrib.slim.separable_conv2d(net, \n",
    "                                                       num_outputs=None, # to skip pointwise stage\n",
    "                                                       kernel_size=[3,3], \n",
    "                                                       stride=layer_def['stride'], \n",
    "                                                       activation_fn=activation_fn,\n",
    "                                                       normalizer_fn=normalizer_fn,\n",
    "                                                       normalizer_params=normalizer_params,\n",
    "                                                       weights_initializer=weights_initializer,\n",
    "                                                       scope=layer_def['name']+'_depthwise')\n",
    "                # pointwise conv\n",
    "                net = tf.contrib.slim.conv2d(net,\n",
    "                                             num_outputs=layer_def['depth']*depth_multiplier,\n",
    "                                             kernel_size=[1,1],\n",
    "                                             activation_fn=activation_fn,\n",
    "                                             normalizer_fn=normalizer_fn,\n",
    "                                             normalizer_params=normalizer_params,\n",
    "                                             weights_initializer=weights_initializer,\n",
    "                                             weights_regularizer=weights_regularizer,\n",
    "                                             scope=layer_def['name']+'_pointwise')\n",
    "                \n",
    "            else:\n",
    "                raise ValueError('Unsupported layer type'+layer_def['layer'])\n",
    "            \n",
    "        # top layers\n",
    "        convout_shape = net.get_shape().as_list()\n",
    "        net = tf.contrib.slim.avg_pool2d(net, [convout_shape[1],convout_shape[2]], padding='VALID', scope='AvgPool')\n",
    "        net = tf.contrib.slim.dropout(net, keep_prob=0.999, is_training=is_training, scope='Dropout')\n",
    "        logits = tf.contrib.slim.conv2d(net, \n",
    "                                num_outputs=num_classes, \n",
    "                                kernel_size=[1,1], \n",
    "                                activation_fn=None,\n",
    "                                normalizer_fn=None, \n",
    "                                scope='Dense')\n",
    "        logits = tf.squeeze(logits, axis=[1,2], name='Squeeze')\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Create Train-Eval Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/winfred/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/winfred/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/winfred/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_0/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_1_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_1_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_2_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_2_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_3_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_3_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_4_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_4_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_5_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_5_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_6_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_6_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_7_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_7_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_8_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_8_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_9_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_9_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_10_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_10_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_11_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_11_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_12_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_12_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_13_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_13_pointwise/add_fold\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'loss' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7dcf8a61122f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mtrain_eval_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-7dcf8a61122f>\u001b[0m in \u001b[0;36mtrain_eval_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexponential_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m450\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstaircase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# create summary and merge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'loss' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def train_eval_loop():\n",
    "    # create a session\n",
    "    sess = tf.InteractiveSession()\n",
    "#     g = tf.get_default_graph()\n",
    "\n",
    "    # create placeholders for images and labels\n",
    "#     images_placeholder = tf.placeholder(tf.float32, shape=[None, 192, 160, 3], name='images')\n",
    "#     labels_placeholder = tf.placeholder(tf.float32, shape=[None, 40], name='labels')\n",
    "\n",
    "    # dataset\n",
    "    ds_iterator = ds_celeba.make_initializable_iterator()\n",
    "    sess.run(ds_iterator.initializer)\n",
    "    \n",
    "    images, labels = ds_iterator.get_next()\n",
    "\n",
    "    # create model\n",
    "    logits = mobilenet_v1(images, num_classes=40, depth_multiplier=0.5, is_training=True)\n",
    "\n",
    "    # define loss\n",
    "    loss_op = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "\n",
    "    # create quantized training graph\n",
    "    tf.contrib.quantize.create_training_graph(quant_delay=0)\n",
    "\n",
    "    # learning rate\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    learning_rate = tf.train.exponential_decay(0.01, global_step, 450, 0.8, staircase=True)\n",
    "    \n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_op)\n",
    "    \n",
    "    # create summary and merge\n",
    "    tf.summary.scalar('loss', loss_op)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    merged_summaries = tf.summary.merge_all()\n",
    "    \n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "    # initialize variables\n",
    "    init = tf.global_variables_initializer()\n",
    "            \n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        print(images)\n",
    "\n",
    "        _, loss, summary = sess.run(\n",
    "            [\n",
    "                train_step,\n",
    "                loss_op,\n",
    "                merged_summaries\n",
    "            ]\n",
    "        )\n",
    "        print('Epoch:', '%04d' % (epoch+1), 'loss=', '{:.9f}'.format(loss))\n",
    "\n",
    "    \n",
    "    \n",
    "train_eval_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
